# tpd_field_checker_step3.py

import os
import pandas as pd

# ---------------------------
# Configuration
# ---------------------------
RUN_CSV_PATH    = r"C:\Input\Run.csv"
FILES_DIRECTORY = r"C:\Files\Located"

# ---------------------------
# Step 1: Read Run.csv
# ---------------------------
def read_run_csv(path):
    """
    Reads Run.csv without header, returns DataFrame or None on failure.
    """
    try:
        df = pd.read_csv(path, header=None, low_memory=False)
        print(f"Loaded Run.csv: {df.shape}")
        print(df.head(3))
        return df
    except Exception as e:
        print(f"Error reading {path}: {e}")
        return None

# ---------------------------
# Step 2: Parse mapping
# ---------------------------
def parse_run_df(df):
    """
    From the raw DataFrame, extract:
    - tpd_fields: list of field names
    - file_map: dict of header -> filename.csv
    - field_mapping: DataFrame of mapping cells
    """
    # TPD fields from column 1, rows 1 onward
    tpd_fields = df.iloc[1:, 1].dropna().astype(str).tolist()
    print(f"Extracted {len(tpd_fields)} TPD fields")

    # Headers for files from row 0, cols 3â€“22
    headers = df.iloc[0, 3:23].dropna().astype(str).tolist()
    file_map = {hdr: f"{hdr}.csv" for hdr in headers}
    print(f"Found {len(file_map)} target files: {list(file_map.values())}")

    # Field mapping matrix
    field_mapping = df.iloc[1:, 3:3 + len(headers)].copy()
    field_mapping.columns = headers
    field_mapping.index   = tpd_fields
    print("Sample mapping (first 3 rows):")
    print(field_mapping.head(3))

    return tpd_fields, file_map, field_mapping

# ---------------------------
# Step 3: Evaluate a single file
# ---------------------------
def evaluate_file(file_path, tpd_fields, file_name, field_mapping):
    """
    For each TPD field, use the mapping to find the actual column in the CSV,
    then check presence and completeness.
    Returns a list of result strings aligned with tpd_fields.
    """
    try:
        df = pd.read_csv(file_path, low_memory=False)
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ["N/A"] * len(tpd_fields)

    # assume data rows start from row 1, and column 0 has an identifier
    total_entries = df.iloc[1:, 0].dropna().shape[0]
    results = []

    for field in tpd_fields:
        actual_field = None
        if field in field_mapping.index and file_name in field_mapping.columns:
            actual_field = field_mapping.at[field, file_name]

        if pd.isna(actual_field) or str(actual_field).strip() == "":
            results.append("Field Missing")
            continue

        if actual_field not in df.columns:
            results.append("Field Missing")
        else:
            filled = df[actual_field].dropna().shape[0]
            if filled == total_entries:
                results.append("Included")
            elif filled > 0:
                results.append("Inconsistent - partial data")
            else:
                results.append("All blank")
    return results

# ---------------------------
# Main flow
# ---------------------------
def main():
    df_run = read_run_csv(RUN_CSV_PATH)
    if df_run is None:
        print("Exiting: could not load Run.csv")
        return

    tpd_fields, file_map, field_mapping = parse_run_df(df_run)

    print("\nEvaluating files:")
    for header, file_name in file_map.items():
        print(f"\n== {file_name} ==")
        file_path = os.path.join(FILES_DIRECTORY, file_name)
        if not os.path.exists(file_path):
            print(f"MISSING: {file_path}")
            continue

        results = evaluate_file(file_path, tpd_fields, header, field_mapping)
        # Print each field and its result
        for fld, res in zip(tpd_fields, results):
            print(f"{fld}: {res}")

if __name__ == "__main__":
    main()

