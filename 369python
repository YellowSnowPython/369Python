# tpd_field_checker.py

import os
import pandas as pd

# ---------------------------
# Configuration
# ---------------------------
RUN_CSV_PATH    = r"C:\Input\Run.csv"
FILES_DIRECTORY = r"C:\Files\Located"
OUTPUT_DIRECTORY = os.path.join(FILES_DIRECTORY, "Output")

# Prepare output directory
if not os.path.exists(OUTPUT_DIRECTORY):
    os.makedirs(OUTPUT_DIRECTORY)
else:
    for fn in os.listdir(OUTPUT_DIRECTORY):
        fp = os.path.join(OUTPUT_DIRECTORY, fn)
        if os.path.isfile(fp):
            try:
                os.remove(fp)
            except Exception as e:
                print(f"Warning: could not remove {fp}: {e}")

# ---------------------------
# Read Run.csv
# ---------------------------
def read_run_csv(path):
    try:
        return pd.read_csv(path, header=None, low_memory=False)
    except Exception as e:
        print(f"Error reading Run.csv: {e}")
        return None

# ---------------------------
# Parse fields + filenames
# ---------------------------
def parse_run_df(df):
    # 1) TPD fields in col B, rows 2+
    tpd_fields = df.iloc[1:, 1].dropna().astype(str).tolist()

    # 2) Headers from row1, files from row2, cols D onward
    raw_headers = df.iloc[0, 3:]
    raw_fnames  = df.iloc[1, 3:]

    # keep only truly non-empty filenames
    mask = raw_fnames.notna() & (raw_fnames.astype(str).str.strip() != "")
    headers    = raw_headers[mask].astype(str).tolist()
    file_names = raw_fnames[mask].astype(str).str.strip().tolist()

    # **OPTIONAL**: make sure they all end in .csv
    file_names = [
        fn if fn.lower().endswith(".csv")
           else fn + ".csv"
        for fn in file_names
    ]

    return tpd_fields, dict(zip(headers, file_names))

# ---------------------------
# Evaluate one data file
# ---------------------------
def evaluate_file(full_path, tpd_fields):
    df = pd.read_csv(full_path, low_memory=False)
    total_rows = df.shape[0]

    results = []
    for field in tpd_fields:
        if field in df.columns:
            non_blank = df[field].dropna()
            non_blank = non_blank[non_blank.astype(str).str.strip() != ""]
            cnt = non_blank.shape[0]

            if cnt == total_rows:
                results.append("Included")
            elif cnt > 0:
                results.append("Partial")
            else:
                results.append("All Blank")
        else:
            results.append("Field Missing")
    return results

# ---------------------------
# Write out a per-file report
# ---------------------------
def save_results(file_name, tpd_fields, results):
    out_df = pd.DataFrame({
        "TPD Field Name": tpd_fields,
        "Status":         results,
    })
    out_df.to_csv(os.path.join(OUTPUT_DIRECTORY, file_name), index=False)

# ---------------------------
# Main
# ---------------------------
def main():
    df_run = read_run_csv(RUN_CSV_PATH)
    if df_run is None:
        return

    tpd_fields, file_map = parse_run_df(df_run)

    # DEBUG: print what we found
    print("Parsed file_map:")
    for hdr, fn in file_map.items():
        print(f"  '{hdr}' → '{fn}'")

    if not file_map:
        print("No files specified in row 2 of Run.csv.")
        return

    for header, fname in file_map.items():
        full_path = os.path.join(FILES_DIRECTORY, fname)
        print(f"Looking for: {full_path}  → exists? {os.path.exists(full_path)}")
        if not os.path.exists(full_path):
            print(f"Missing file for header '{header}': {fname}")
            continue

        results = evaluate_file(full_path, tpd_fields)
        save_results(fname, tpd_fields, results)
        print(f"Results written for file: {fname}")

if __name__ == "__main__":
    main()
